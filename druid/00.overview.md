# Druid Overview

드루이드는 이벤트 데이터에 대한 **OLAP** 쿼리를 위해 설계된 오픈 소스 데이터 저장소이다.

## Data

드루이드에서 데이터는 3가지 개념으로 나눈다.

- Timestamp column

  드루이드에서 모든 질의는 timestamp기반으로 처리한다.

- Dimension column

- 필터링하거나 그룹바이할 떄 쓰이는 문자열 컬럼

- Metric column

- 집계 결과를 위한 컬럼, 보통 숫자형의 데이터 컬럼이 됨. count, sum, mean 등 연산이 가능함

## Sharding the Data

드루이드는 항상 먼저 시간에 대해서 데이터를 샤딩한다.

예를 들면 다음과 같이 시간에 대해서 데이터가 각 세그먼트로 분리된다.

세그먼트 이름은 `dataSource_interval_version_partitionNumber` 이런식으로 결정 된다.

## Roll-up

원시 데이터를 줄이기 위해 수집될 때 롤업을 함

드루이드는 2가지 롤업 모드를 제공한다.

1. perfect roll-up
   처리 시간에 완벽하게(모든 데이터를) 집게됨
   이 모드에서는 실제초 집계를 하기 전에 전처리 과정이 필요함(intervals and shardSpecs 이런것들을 결정하기 위해)
2. best-effort roll-up
    Meanwhile, in the best-effort roll-up, input data might not be perfectly aggregated and thus there can be multiple segments holding the rows which should belong to the same segment with the perfect roll-up since they have the same dimension value and their timestamps fall into the same interval.
      전처리 과정이 필요하지는 않지만 데이터가 perfect roll-up mode 보다 크게 나올 수 있음
      모든 streaming indexing(실시간 인덱스 생성, 카프카 인덱싱 서비스)은 이 모드로 처리됨

## Indexing the Data

데이터의 immutable한 snapshot을 만들어서 쿼리에 사용한다.

드루이드는 컬럼을 저장하는데 이것은 컬럼이 각각 따로 저장될 수 있다는것을 의미함

쿼리에 필요한 컬럼을 사용하는데, 드루이드는 이 컬럼을 찾는것을 매우 잘함

## Loading the Data

드루이드의 Igeation은 두가지임

1. Real Time
2. Batch

## Querying the Data

드루이드의 기본 쿼리 언어는 `HTTP + JSON` 이지만 라이브러리를 사용하면 SQL 등 다양한 언어로 사용할 수 있음

드루이드는 조인이 없다. (하지만 듣기로 join이 가능하게 extension을 **만들어서** 쓰면 된다고 함.)

드루이드에 로드하기 전 데이터를 비정규화 해야하므로 ETL에서 조인을 수행해야함

## The Druid Cluster

- Historical Nodes
  드루이드 클러스터의 백본 역할
  Immutable Segment를 로컬에서 다운로드하고 그 세그먼트에 대한 쿼리를 제공함
  load segment, drop segment, serve query on segment
- Broker Nodes
  클라이언트와 응용 프로그램이 드루이드에서 데이터를 얻기위해 질의하는 
  **쿼리 분산 및 결과 수집 및 병합**을 담당함
  세그먼트가 어디에 있는지 알 수 있음
- Coordinator Nodes
  클러스터의 Historical Nodes에 있는 **세그먼트를 관리함**
  Historical Nodes에 새로운 세그먼트를 로드하고, 옛날 세그먼트를 삭제하고, 세그먼트들을 이동시킴(load balance)
- Real-time Processing
  Standalone Real-time node나 Indexing Service를 사용해서 실시간 처리를 할 수 있음
  Real-time processing(데이터를 ingeation하고 세그먼트를 만듬)
  Hand-off segment(Historical 노드에게 세그먼트 전달)

## External Dependencies

- Zookeeper
  Service discovery
- Metadata Storage
  Segment나 설정에 대한 metadata를 저장하는 곳
  Segment를 생성하면 Metadata Storage에 새 항목을 쓰면,
  Coordinator Node에서 그것을 보고 새로운 데이터가 저장되는지 옛날 데이터가 삭제되는지 알 수 있다.
  Production으로는 MySQL, PostgreSQL
  테스트용 단일 머신에서 Derby도 괜찮음
- Deep Storage
  Segment의 영구적인 백업 저장소
  Segment를 생성하면 Segment를 Deep Storage에 업로드하고
  Historical Node는 Deep Storage에서 Segment를 다운로드함
  
## High Availability 

기본적으로 HA를 보장하게 설계되었다함.

걱정없이 쓰고싶다면 각 노드 타입당 최소 2개이상으로

## refer

-[http://druid.io/docs/latest/design/](http://druid.io/docs/latest/design/)
